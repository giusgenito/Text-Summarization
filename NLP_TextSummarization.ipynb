{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGW6XLtFxre5"
      },
      "source": [
        "# Progetto NLP Text summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'obiettivo di questo progetto è la creazione di un modello di NLP in grado di **sintetizzare testi lunghi**, scritti in italiano, in poche righe di testo. Per farlo è stato necessario estrarre degli articoli tecnologici della rivista **IlPost**, successivamente è stato utilizzato un modello pre-addestrato **MBART** per continuare l'addestramento sui 160 articoli da noi estratti. Successivamente sono state calcolate le misure di accuratezza, e per concludere viene mostrato l'utilizzo del modello in un caso reale."
      ],
      "metadata": {
        "id": "dLRxYZDl88BC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-O7yts1ydlv"
      },
      "source": [
        "## **Librerie e Drive**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZveKYCSy8Nn"
      },
      "source": [
        "#### Installazioni librerie esterne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58-EK61gy44P"
      },
      "outputs": [],
      "source": [
        "!pip install textacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr5a6dYTOozg"
      },
      "outputs": [],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgaR5cIUvmZi"
      },
      "outputs": [],
      "source": [
        "!pip install SentencePiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAbJayKfzBqp"
      },
      "source": [
        "### Librerie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGKWC2Zfx4KF"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf')\n",
        "from matplotlib.colors import to_rgba\n",
        "from textacy import preprocessing\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AdamW\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import SmoothingFunction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfv_k2XAyqk3"
      },
      "source": [
        "### Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgBiTrcYyscn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OlnBjXzx3sf"
      },
      "source": [
        "## **Creazione dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scraping pagine web"
      ],
      "metadata": {
        "id": "W7Pfb_Y1ly7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il sito web IlPost è composto da varie sezioni, l'estrazione dei dati per la creazione del dataset avviene nell'ambito tecnologia. In ogni pagina della sezione Tecnologia sono presenti 20 articoli, lo scopo è estrarre il titolo principale, il titolo secondario e i paragrafi di ogni articolo. Il dataset è composto da due colonne, source e target, corrispettivamente saranno il testo lungo formato dall'unione di tutti i paragrafi di un articolo, e l'unione di titolo primario e secondario."
      ],
      "metadata": {
        "id": "UC5fYxOKl1ob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mnVqqvF9emW"
      },
      "outputs": [],
      "source": [
        "def coppie(url):\n",
        "  time.sleep(1)\n",
        "  r = requests.get(url)\n",
        "  soup = BeautifulSoup(r.text, \"html5lib\")\n",
        "  articles = soup.find_all(\"article\", class_=\"_taxonomy-item_q6jgq_1 _opener_q6jgq_14\")\n",
        "  # list of summaries\n",
        "  summary = []\n",
        "  for i in range(len(articles)):\n",
        "    try:\n",
        "      testo1 = articles[i].select_one(\"a\").get_text().strip()\n",
        "    except:\n",
        "      testo1 = \"\"\n",
        "    try:\n",
        "      testo2 = articles[i].select_one(\"p\").get_text().strip()\n",
        "    except:\n",
        "      testo2 = \"\"\n",
        "\n",
        "    if testo1[-1] == \"?\":\n",
        "      if testo2 == \"\": riassunto = testo1\n",
        "      elif testo2[-1] == \".\": riassunto = testo1 + \" \" + testo2\n",
        "      else: riassunto = testo1 + \" \" + testo2[0].lower() + testo2[1:] + \".\"\n",
        "    elif testo1[-1] == \",\":\n",
        "      if testo2 == \"\": riassunto = testo1[:-1] + \".\"\n",
        "      elif testo2[-1] == \".\": riassunto = testo1 + \" \" + testo2[0].lower() + testo2[1:]\n",
        "      else: riassunto = testo1 + \" \" + testo2[0].lower() + testo2[1:] + \".\"\n",
        "    elif testo1[-1] != \",\":\n",
        "      if testo2 == \"\": riassunto = testo1 + \".\"\n",
        "      elif testo2[-1] == \".\": riassunto = testo1 + \". \" + testo2\n",
        "      else: riassunto = testo1 + \". \" + testo2 + \".\"\n",
        "\n",
        "    summary.append((riassunto))\n",
        "\n",
        "  # articles urls\n",
        "  pagelinks = []\n",
        "  print(\"lunghezza articoli: \", len(articles))\n",
        "\n",
        "  for i in range(len(articles)):\n",
        "    url = articles[i].find_all('a')[0]\n",
        "    pagelinks.append(url.get('href'))\n",
        "\n",
        "  thearticle = []\n",
        "  paragraph_test = []\n",
        "  print(\"lunghezza pagelinks: \", len(pagelinks))\n",
        "  for link in pagelinks:\n",
        "    paragraphtext = []\n",
        "    url = link\n",
        "    page = requests.get(url)\n",
        "    time.sleep(0.2)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "\n",
        "    testo_articolo = soup.find_all(\"p\")\n",
        "    for paragraph in testo_articolo[:-1]:\n",
        "        text = paragraph.get_text()\n",
        "        paragraphtext.append(text)\n",
        "\n",
        "    testo_fin = ' '.join(paragraphtext)\n",
        "    thearticle.append(testo_fin)\n",
        "  d = {'source': thearticle, 'target': summary}\n",
        "  return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kq7HR2EIMV09"
      },
      "outputs": [],
      "source": [
        "df_train = pd.DataFrame(columns=[\"source\", \"target\"])\n",
        "df_test = pd.DataFrame(columns=[\"source\", \"target\"])\n",
        "i = 0\n",
        "j = 0\n",
        "conto = 0\n",
        "argomenti = \"tecnologia\"\n",
        "numero_articoli = 160\n",
        "numero_articoli_per_pagina = 20\n",
        "numero_test = 3                 # ogni unità equivale a 20 articoli, però partendo da 2 in su.\n",
        "\n",
        "for j in range(1, int(numero_articoli/numero_articoli_per_pagina)+1):\n",
        "  if j == 1:\n",
        "    ret = coppie(\"https://www.ilpost.it/\" + argomenti+\"/\")\n",
        "    conto += 20\n",
        "    print(\"\\nPagina: \" + \"https://www.ilpost.it/\" + argomenti+\"/\"\n",
        "    + \"\\nArticoli Scaricati: \" + str(conto))\n",
        "    df_train = pd.concat([df_train, pd.DataFrame(ret)])\n",
        "\n",
        "  else:\n",
        "    ret = coppie(\"https://www.ilpost.it/\" + argomenti + \"/page/\" + str(j) +\"/\")\n",
        "    conto += 20\n",
        "    print(\"\\nPagina: \" + \"https://www.ilpost.it/\" + argomenti + \"/page/\" + str(j) +\"/\"\n",
        "    + \"\\nArticoli Scaricati: \" + str(conto))\n",
        "    df_train = pd.concat([df_train, pd.DataFrame(ret)])\n",
        "\n",
        "for j in range(int(numero_articoli/numero_articoli_per_pagina)+1, int(numero_articoli/numero_articoli_per_pagina)+numero_test):\n",
        "    ret = coppie(\"https://www.ilpost.it/\" + argomenti + \"/page/\" + str(j) +\"/\")\n",
        "    print(\"\\nPagina: \" + \"https://www.ilpost.it/\" + argomenti + \"/page/\" + str(j) +\"/\")\n",
        "    df_test = pd.concat([df_test, pd.DataFrame(ret)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cleaning"
      ],
      "metadata": {
        "id": "53gaUa7ZojvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All'interno della funzione coppie avviene già una correzione quando titolo principale e secondario vengono uniti, per far si che il titolo principale termina con un punto e dopo lo spazio inizi il secondario. Questo cunck ha lo scopo di eliminare eventuali articoli che non sono stati catturati dallo scraping e quindi vengono segnati come vuoti."
      ],
      "metadata": {
        "id": "6rnus4fdolDg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hdvf1HPzenJ"
      },
      "outputs": [],
      "source": [
        "# some cleaning, remove empty rows\n",
        "df_train['source'].replace(\"\", np.nan, inplace=True)\n",
        "df_train['target'].replace(\"\", np.nan, inplace=True)\n",
        "df_train.dropna(subset=['source'], inplace=True)\n",
        "df_train.dropna(subset=['target'], inplace=True)\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "\n",
        "print(df_train.source.size, df_train.target.size)\n",
        "\n",
        "# some cleaning, remove empty rows\n",
        "df_test['source'].replace(\"\", np.nan, inplace=True)\n",
        "df_test['target'].replace(\"\", np.nan, inplace=True)\n",
        "df_test.dropna(subset=['source'], inplace=True)\n",
        "df_test.dropna(subset=['target'], inplace=True)\n",
        "df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "print(df_test.source.size, df_test.target.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlA1j43bMBtu"
      },
      "outputs": [],
      "source": [
        "df_train.to_csv(\"/content/drive/MyDrive/Prog_NLP/train.csv\", sep=',', index=False)\n",
        "df_test.to_csv(\"/content/drive/MyDrive/Prog_NLP/test.csv\", sep=',', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4zKwXZ80jl_"
      },
      "source": [
        "#### Plot lunghezze riassunti"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizziamo attraverso un istogramma quante parole ci sono nelle sintesi composte dall'unione del titolo primario e secondario. Noteremo due curve dovute dal fatto che non tutti gli articoli dispongono del titolo secondario e dunque vi sarà una netta separazione tra la lunghezza di articoli con solo il titolo principale e quelli con titolo principale e secondario."
      ],
      "metadata": {
        "id": "uBPhg8lInJPu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub-KEnG2zzgv"
      },
      "outputs": [],
      "source": [
        "word_bound = re.compile(r'\\b')\n",
        "\n",
        "def num_words(line):\n",
        "    return len(word_bound.findall(line)) >> 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFs_UWri0nZJ"
      },
      "outputs": [],
      "source": [
        "ii = [i for i in range(0,80)]\n",
        "lunghezze_riass = [num_words(df_train[\"target\"].iloc[i]) for i in range(df_train.shape[0])]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot()\n",
        "fig.subplots_adjust(top=0.85)\n",
        "\n",
        "fig.suptitle('lunghezze riassunti training set', fontsize=14, fontweight='bold')\n",
        "ax.set_title('IlPost')\n",
        "\n",
        "ax.set_xlabel('tokens')\n",
        "ax.set_ylabel('numero')\n",
        "\n",
        "#plt.style.use(['dark_background'])\n",
        "plt.hist(lunghezze_riass, bins=ii, color='c')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nel prossimo grafico viene mostrata la lunghezza dei testi in numero di parole degli articoli formati dall'unione di tutti i paragrafi."
      ],
      "metadata": {
        "id": "ptbWCtWKnsoT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3JKGf2f09E9"
      },
      "outputs": [],
      "source": [
        "ii = [i for i in range(0,500,6)]\n",
        "lunghezze_docu = [num_words(df_train['source'].iloc[i]) for i in range(df_train.shape[0])]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot()\n",
        "fig.subplots_adjust(top=0.85)\n",
        "\n",
        "fig.suptitle('lunghezze documenti', fontsize=14, fontweight='bold')\n",
        "ax.set_title('IlPost')\n",
        "\n",
        "ax.set_xlabel('tokens')\n",
        "ax.set_ylabel('numero')\n",
        "\n",
        "#plt.style.use(['dark_background'])\n",
        "plt.hist(lunghezze_docu, bins=ii, color='c')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-processing"
      ],
      "metadata": {
        "id": "54dPB1wh-ZnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessaria prima della tokenizzazione una fase di pre processing dove il testo viene normalizzato attraverso la standardizzazione dei caratteri, vi è poi la rimozione dei tag di HTML, e la rimozione dei spazi bianchi."
      ],
      "metadata": {
        "id": "SAUknDHdn3Cq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ry2Vebf1EtM"
      },
      "outputs": [],
      "source": [
        "## Pre processing training set\n",
        "df_train = pd.read_csv(\"/content/drive/MyDrive/Prog_NLP/train.csv\")\n",
        "df_train.shape[0]\n",
        "clean_inputs = []\n",
        "clean_targets = []\n",
        "\n",
        "preproc = preprocessing.make_pipeline(\n",
        "     preprocessing.normalize.unicode,\n",
        "     preprocessing.remove.html_tags,\n",
        "     preprocessing.normalize.whitespace\n",
        ")\n",
        "\n",
        "# apply to dataset elements\n",
        "clean_inputs = [preproc(df_train[\"source\"].iloc[i]) for i in range(df_train.shape[0])]\n",
        "clean_targets = [preproc(df_train[\"target\"].iloc[i]) for i in range(df_train.shape[0])]\n",
        "df_train = pd.DataFrame({\"source\": clean_inputs, \"target\": clean_targets})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYfJQ2MY1r-D"
      },
      "outputs": [],
      "source": [
        "## Pre processing test set\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/Prog_NLP/test.csv\")\n",
        "df_test.shape[0]\n",
        "clean_inputs = []\n",
        "clean_targets = []\n",
        "\n",
        "preproc = preprocessing.make_pipeline(\n",
        "     preprocessing.normalize.unicode,\n",
        "     preprocessing.remove.html_tags,\n",
        "     preprocessing.normalize.whitespace\n",
        ")\n",
        "\n",
        "# apply to dataset elements\n",
        "clean_inputs = [preproc(df_test[\"source\"].iloc[i]) for i in range(df_test.shape[0])]\n",
        "clean_targets = [preproc(df_test[\"target\"].iloc[i]) for i in range(df_test.shape[0])]\n",
        "df_test = pd.DataFrame({\"source\": clean_inputs, \"target\": clean_targets})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEVpGgcM1j1b"
      },
      "outputs": [],
      "source": [
        "df_train.to_csv(\"/content/drive/MyDrive/Prog_NLP/train_clean.csv\", sep=',', index=False)\n",
        "df_test.to_csv(\"/content/drive/MyDrive/Prog_NLP/test_clean.csv\", sep=',', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfS0GWi_u6vM"
      },
      "source": [
        "## **Addestramento modello**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questa parte di codice viene importato il modello pre-addestrato MBART dalla pagina web HuggingFace, al link: https://huggingface.co/ARTeLab/mbart-summarization-ilpost.\n",
        "\n",
        "Successivamente viene addestrato attraverso il dataset costruito nei cunck precedenti attraverso lo scraping della pagina web IlPost."
      ],
      "metadata": {
        "id": "-HsofEW6pQsc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iICnp9FLMiIb"
      },
      "source": [
        "#### Importiamo il modello pre-addestrato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUg3-40kvHpC"
      },
      "outputs": [],
      "source": [
        "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
        "tokenizer = MBartTokenizer.from_pretrained(\"ARTeLab/mbart-summarization-ilpost\")\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"ARTeLab/mbart-summarization-ilpost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Addestramento del modello"
      ],
      "metadata": {
        "id": "ResnkptDqBbp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daXlo3yNBMBX"
      },
      "source": [
        "Iper-prametri utilizzati nei try\n",
        "\n",
        "Sono stati segnati gli iperparametri modificati durante l'addestramento, il numero limitato di tentativi di fine-tuning è dovuto dal temine dei crediti di Colab.\n",
        "\n",
        "**Primo Dataset 80 train, 20 test**\n",
        "\n",
        "1| batch=8, lr=1e-6, epoch=5\n",
        "\n",
        "2| batch=16, lr=1e-6, epoch=5\n",
        "\n",
        "3| batch=4, lr=1e-6, epoch=5\n",
        "\n",
        "4| batch=2, lr=1e-6, epoch=5\n",
        "\n",
        "5| batch=2, lr=1e-6, epoch=10 BEST TRY\n",
        "\n",
        "**Nuovo Dataset 160 train, 40 test**\n",
        "\n",
        "6| batch=2, lr=1e-6, epoch=10 BEST TRY\n",
        "\n",
        "7| batch=2, lr=1e-5, epoch=10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpmsFLIajkNp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/Prog_NLP/train_clean.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Prog_NLP/test_clean.csv\")\n",
        "\n",
        "train_texts = list(train[\"source\"])\n",
        "train_targets = list(train[\"target\"])\n",
        "\n",
        "test_texts = list(test[\"source\"])\n",
        "test_targets = list(test[\"target\"])\n",
        "\n",
        "\n",
        "# Tokenizzazione dei dati\n",
        "\n",
        "train_inputs = tokenizer(train_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "train_labels = tokenizer(train_targets, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "test_inputs = tokenizer(test_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "test_labels = tokenizer(test_targets, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "\n",
        "# Crea il dataset PyTorch\n",
        "train_dataset = TensorDataset(train_inputs[\"input_ids\"], train_labels[\"input_ids\"])\n",
        "test_dataset = TensorDataset(test_inputs[\"input_ids\"], test_labels[\"input_ids\"])\n",
        "# Crea il dataloader per il training\n",
        "batch_size = 2\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Definisci l'ottimizzatore, la loss function e altri parametri di addestramento\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "loss_media_train = []\n",
        "\n",
        "# Addestra il modello\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
        "        inputs = batch[0]\n",
        "        labels = batch[1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "    loss_media_train.append(average_loss)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss:.4f}')\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs = batch[0]\n",
        "        labels = batch[1]\n",
        "\n",
        "        outputs = model.generate(inputs)\n",
        "\n",
        "        # Decodifica le predizioni e i riferimenti\n",
        "        predicted_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        target_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        # Aggiungi le predizioni e i riferimenti alle liste\n",
        "        predictions.extend(predicted_texts)\n",
        "        references.extend(target_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNl2pZESTl5u"
      },
      "outputs": [],
      "source": [
        "# Salvataggio del modello\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Prog_NLP/tesxsumm_bert.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjhIKcIlHdww"
      },
      "source": [
        "#### **Grafico andamento loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzClVqspTR4D"
      },
      "source": [
        "#### **Curve di Loss**\n",
        "\n",
        "Questo ti darà un'indicazione di come la loss cambia durante l'addestramento per il set di training. La curva di loss del training decresce costantemente.\n",
        "Questo indica che il modello sta imparando bene dai dati di training. Purtroppo la curva non arriva in un punto in cui termina di decrescere, ma a causa della limitazione nei crediti di Colab non è stato possibile utilizzare l'early stopping per terminare l'addestramento quando la loss per le epoche era la stessa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5QKK44TTJsq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Lista della loss del training set\n",
        "loss_train = loss_media_train\n",
        "\n",
        "# Plot della loss\n",
        "plt.figure(figsize=(10, 6))  # Imposta le dimensioni della figura\n",
        "\n",
        "# Imposta lo stile della linea senza marker\n",
        "plt.plot(loss_train, linestyle='-', color='b', label='Loss training set')\n",
        "\n",
        "# Aggiungi titoli e etichette agli assi\n",
        "plt.title('Andamento della loss nelle epoche', fontsize=16)\n",
        "plt.xlabel('Loss', fontsize=14)\n",
        "plt.ylabel('Numero epoche', fontsize=14)\n",
        "\n",
        "# Aggiungi una griglia per facilitare la lettura del grafico\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Aggiungi una legenda\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Personalizza i tick degli assi\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Aggiungi una linea orizzontale per evidenziare la loss media\n",
        "loss_media = sum(loss_train) / len(loss_train)\n",
        "plt.axhline(y=loss_media, color='r', linestyle='--', label='Media delle Loss')\n",
        "\n",
        "# Aggiungi testo per indicare la media delle loss\n",
        "plt.text(len(loss_train) - 1, loss_media, f'Media: {loss_media:.2f}', ha='right', va='bottom', color='r', fontsize=12)\n",
        "\n",
        "# Mostra il grafico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EecftUbPTWNe"
      },
      "source": [
        "#### **Distribuzione delle Lunghezze dei Riassunti:**\n",
        "\n",
        "Plot dell'istogramma delle lunghezze dei riassunti generati e dei riferimenti per vedere come sono distribuiti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G67PRe2OTN0_"
      },
      "outputs": [],
      "source": [
        "# Calcola le lunghezze dei riassunti generati e dei riferimenti\n",
        "generated_lengths = [len(text.split()) for text in predictions]\n",
        "reference_lengths = [len(text.split()) for text in references]\n",
        "\n",
        "# Plotta l'istogramma delle lunghezze\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(generated_lengths, bins=20, alpha=0.5, label='Generated Summaries')\n",
        "plt.hist(reference_lengths, bins=20, alpha=0.5, label='Reference Summaries')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Summary Lengths')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7pTMXzxHj0Y"
      },
      "source": [
        "## **Misure di accuratezza**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcoliamo le misure di accuratezza del modello"
      ],
      "metadata": {
        "id": "bfnoBDcVq8gC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekfs8GMzIJEL"
      },
      "source": [
        "#### **ROUGE**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La misura ROUGE in questo caso è formata da ROUGE-1, ROUGE-2 e ROUGE-L, che ROUGE 1 e 2 indica la sovrapposizione degli n-grammi, quindi nel caso in questione dell'unigramma e del bigramma. ROUGE-L invece riguarda la lunghezza delle sequenze"
      ],
      "metadata": {
        "id": "NYMUjaK0rA6B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex8ELM2KmlYD"
      },
      "outputs": [],
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(predictions, references, avg=True)\n",
        "\n",
        "# Stampa i risultati\n",
        "print(\"ROUGE Scores:\")\n",
        "print(f\"ROUGE-1: {scores['rouge-1']}\")\n",
        "print(f\"ROUGE-2: {scores['rouge-2']}\")\n",
        "print(f\"ROUGE-L: {scores['rouge-l']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ijiga0_IKXY"
      },
      "source": [
        "#### **BLUE**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La metrica BLUE non è propriamente adatta alla text summarization, è utilizzata in modo efficace nella traduzione, perché il testo generato dal modello deve corrispondere in modo fedele al testo reale, ma per la sintesi dei testi questo non vale, perché in parole diverse si può esprimere lo stesso significato."
      ],
      "metadata": {
        "id": "go1t8dCgrSvq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01YbsjULH6FU"
      },
      "outputs": [],
      "source": [
        "# Calcola BLEU per ogni coppia di predizione e riferimento con smoothing\n",
        "smoother = SmoothingFunction().method1  # Puoi sperimentare con metodi diversi\n",
        "bleu_scores_smoothed = [sentence_bleu([ref.split()], pred.split(), smoothing_function=smoother) for ref, pred in zip(references, predictions)]\n",
        "\n",
        "# Calcola BLEU per l'intero corpus con smoothing\n",
        "corpus_bleu_score_smoothed = corpus_bleu([[ref.split()] for ref in references], [pred.split() for pred in predictions], smoothing_function=smoother)\n",
        "\n",
        "# Visualizza i risultati con smoothing\n",
        "print(f\"BLEU Scores with Smoothing (per ogni coppia): {bleu_scores_smoothed}\")\n",
        "print(f\"BLEU Score with Smoothing (intero corpus): {corpus_bleu_score_smoothed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFAa8WaXHqhG"
      },
      "source": [
        "## **Esempio di text summarization**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esempio dell'utilizzo del modello di text summary su un articolo della rivista IlPost in ambito tecnologia, non presente nel dataset né di train né di test."
      ],
      "metadata": {
        "id": "fCL7quiJrqbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQMPalYG_6Wk"
      },
      "outputs": [],
      "source": [
        "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
        "tokenizer = MBartTokenizer.from_pretrained(\"ARTeLab/mbart-summarization-ilpost\")\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"ARTeLab/mbart-summarization-ilpost\")\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Prog_NLP/tesxsumm_bert.pkl'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/Prog_NLP/Tastiera.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    input_text = file.read()\n",
        "\n",
        "\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)['input_ids']\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    summary_ids = model.generate(input_ids)\n",
        "\n",
        "# Decodifica il riassunto\n",
        "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Stampa il riassunto\n",
        "print(\"Riassunto generato:\")\n",
        "print(summary_text)"
      ],
      "metadata": {
        "id": "o5H5dEHW9kuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PZveKYCSy8Nn",
        "AAbJayKfzBqp",
        "hfv_k2XAyqk3",
        "3OlnBjXzx3sf",
        "ResnkptDqBbp"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}